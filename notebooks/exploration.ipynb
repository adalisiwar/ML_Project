{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse Comportementale Clientèle Retail - Exploration et Préparation des Données\n",
        "\n",
        "Ce notebook Jupyter (`notebooks/exploration.ipynb`) est dédié à la phase d'exploration et de préparation des données pour le projet d'analyse comportementale clientèle retail. Nous analyserons la qualité du dataset (valeurs manquantes, aberrantes, corrélations), corrigerons les problèmes identifiés, et préparerons les données pour les étapes suivantes (préprocessing avancé, modélisation).\n",
        "\n",
        "Le dataset contient 52 features issues de transactions réelles d'une entreprise e-commerce de cadeaux. L'objectif est de nettoyer et explorer les données pour améliorer la qualité avant la modélisation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports et Configuration\n",
        "\n",
        "Nous importons les bibliothèques nécessaires pour l'analyse de données, la visualisation et le preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports des bibliothèques\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration pour les visualisations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chargement des Données\n",
        "\n",
        "Chargeons le dataset brut depuis le dossier `data/raw/`. Assurez-vous que le fichier est présent (ex. : `customer_behavior_data.csv`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chemin vers le dataset brut\n",
        "data_path = '../data/raw/customer_behavior_data.csv'\n",
        "\n",
        "# Chargement du dataset\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Aperçu des premières lignes\n",
        "print(\"Aperçu du dataset :\")\n",
        "print(df.head())\n",
        "\n",
        "# Informations générales sur le dataset\n",
        "print(\"\\nInformations sur le dataset :\")\n",
        "print(df.info())\n",
        "\n",
        "# Statistiques descriptives\n",
        "print(\"\\nStatistiques descriptives :\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyse de la Qualité des Données\n",
        "\n",
        "### 1. Valeurs Manquantes\n",
        "\n",
        "Analysons la présence de valeurs manquantes dans le dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pourcentage de valeurs manquantes par colonne\n",
        "missing_values = df.isnull().sum() / len(df) * 100\n",
        "print(\"Pourcentage de valeurs manquantes par colonne :\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Visualisation des valeurs manquantes\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title('Heatmap des Valeurs Manquantes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Valeurs Aberrantes (Outliers)\n",
        "\n",
        "Détectons les outliers à l'aide de boxplots et de méthodes statistiques (IQR)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sélection des colonnes numériques\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Boxplots pour détecter les outliers\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols[:10]):  # Limiter à 10 pour lisibilité\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    sns.boxplot(y=df[col])\n",
        "    plt.title(f'Boxplot de {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Fonction pour détecter les outliers via IQR\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "# Exemple : Détection d'outliers pour 'Age' et 'MonetaryTotal'\n",
        "for col in ['Age', 'MonetaryTotal']:\n",
        "    outliers = detect_outliers_iqr(df, col)\n",
        "    print(f\"Nombre d'outliers dans {col} : {len(outliers)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Corrélations\n",
        "\n",
        "Analysons les corrélations entre les features pour détecter la multicolinéarité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrice de corrélation\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Heatmap de corrélation\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "plt.title('Heatmap de Corrélation')\n",
        "plt.show()\n",
        "\n",
        "# Détection de multicolinéarité via VIF (Variance Inflation Factor)\n",
        "def calculate_vif(data):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = data.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(len(data.columns))]\n",
        "    return vif_data\n",
        "\n",
        "# Calculer VIF pour les colonnes numériques (attention : peut être lent avec 52 features)\n",
        "vif_df = calculate_vif(df[numeric_cols].dropna())\n",
        "print(\"VIF pour les features numériques :\")\n",
        "print(vif_df[vif_df['VIF'] > 10])  # VIF > 10 indique multicolinéarité"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correction des Problèmes de Qualité\n",
        "\n",
        "### 1. Imputation des Valeurs Manquantes\n",
        "\n",
        "Imputons les valeurs manquantes en utilisant des méthodes appropriées (KNN pour numériques, médiane pour catégorielles)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imputation pour colonnes numériques (ex. : Age avec KNN)\n",
        "knn_imputer = KNNImputer(n_neighbors=5)\n",
        "df[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Imputation pour colonnes catégorielles (ex. : Satisfaction avec médiane si numérique, sinon mode)\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in categorical_cols:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        mode_val = df[col].mode()[0]\n",
        "        df[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "print(\"Vérification après imputation :\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Correction des Valeurs Aberrantes\n",
        "\n",
        "Corrigeons les outliers en les remplaçant par des valeurs limites (winsorizing) ou en les supprimant si nécessaire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fonction pour winsorizing (remplacer outliers par percentiles)\n",
        "def winsorize(data, column, lower_percentile=0.05, upper_percentile=0.95):\n",
        "    lower_bound = data[column].quantile(lower_percentile)\n",
        "    upper_bound = data[column].quantile(upper_percentile)\n",
        "    data[column] = np.where(data[column] < lower_bound, lower_bound, data[column])\n",
        "    data[column] = np.where(data[column] > upper_bound, upper_bound, data[column])\n",
        "    return data\n",
        "\n",
        "# Appliquer winsorizing à des colonnes sensibles (ex. : SupportTickets, Satisfaction)\n",
        "for col in ['SupportTickets', 'Satisfaction']:\n",
        "    df = winsorize(df, col)\n",
        "\n",
        "print(\"Statistiques après correction des outliers :\")\n",
        "print(df[['SupportTickets', 'Satisfaction']].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Suppression de Features Inutiles\n",
        "\n",
        "Supprimons les features constantes ou avec trop de multicolinéarité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supprimer les colonnes constantes (variance nulle)\n",
        "constant_cols = [col for col in df.columns if df[col].nunique() == 1]\n",
        "df.drop(columns=constant_cols, inplace=True)\n",
        "print(f\"Colonnes constantes supprimées : {constant_cols}\")\n",
        "\n",
        "# Supprimer les colonnes avec VIF > 10 (exemple : si multicolinéarité détectée)\n",
        "high_vif_cols = vif_df[vif_df['VIF'] > 10]['feature'].tolist()\n",
        "df.drop(columns=high_vif_cols, inplace=True)\n",
        "print(f\"Colonnes avec haute multicolinéarité supprimées : {high_vif_cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploration et Préparation des Données\n",
        "\n",
        "### 1. Analyse Exploratoire Supplémentaire\n",
        "\n",
        "Visualisons les distributions et relations clés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distributions des variables numériques\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numeric_cols[:6]):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    sns.histplot(df[col], kde=True)\n",
        "    plt.title(f'Distribution de {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyse de la variable cible (ex. : Churn si présente)\n",
        "if 'Churn' in df.columns:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x='Churn', data=df)\n",
        "    plt.title('Distribution de Churn')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Encodage des Variables Catégorielles\n",
        "\n",
        "Préparons l'encodage pour les phases suivantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-Hot Encoding pour les catégorielles (exemple)\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "encoded_cols = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))\n",
        "encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)\n",
        "df = pd.concat([df.drop(columns=categorical_cols), encoded_cols], axis=1)\n",
        "\n",
        "print(\"Dataset après encodage :\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Normalisation et Split Train/Test\n",
        "\n",
        "Normalisons et préparons le split (attention : appliquer après split pour éviter data leakage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split train/test (stratifié si Churn existe)\n",
        "if 'Churn' in df.columns:\n",
        "    X = df.drop('Churn', axis=1)\n",
        "    y = df['Churn']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "else:\n",
        "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalisation sur train\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
        "\n",
        "print(\"Taille des ensembles :\")\n",
        "print(f\"Train : {X_train_scaled.shape}, Test : {X_test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Réduction de Dimension (ACP)\n",
        "\n",
        "Appliquons une ACP pour visualisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ACP sur les données d'entraînement\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], alpha=0.5)\n",
        "plt.title('ACP - 2 Composantes Principales')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Variance expliquée par les 2 premières composantes : {pca.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sauvegarde des Données Préparées\n",
        "\n",
        "Sauvegardons les données nettoyées pour les phases suivantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarde dans data/processed/\n",
        "df.to_csv('../data/processed/cleaned_data.csv', index=False)\n",
        "X_train_scaled.to_csv('../data/train_test/X_train.csv', index=False)\n",
        "X_test_scaled.to_csv('../data/train_test/X_test.csv', index=False)\n",
        "if 'Churn' in df.columns:\n",
        "    y_train.to_csv('../data/train_test/y_train.csv', index=False)\n",
        "    y_test.to_csv('../data/train_test/y_test.csv', index=False)\n",
        "\n",
        "print(\"Données sauvegardées avec succès !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Ce notebook a permis d'explorer et de préparer le dataset en correctant les problèmes de qualité (valeurs manquantes, aberrantes, multicolinéarité) et en préparant les données pour les étapes de modélisation. Les données nettoyées sont maintenant prêtes pour le preprocessing avancé et la modélisation dans les notebooks suivants."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
